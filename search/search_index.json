{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#uber-mich","title":"\u00dcber mich","text":"<p>Hallo! Mein Name ist Mike Ditton, und ich bin stolz darauf, bei der kantonalen Verwaltung des Kantons Aargau als lernender Informatiker EFZ in der Fachrichtung Plattformentwicklung t\u00e4tig zu sein. Hier absolviere ich meine Berufslehre und sammle wertvolle Erfahrungen, um meine F\u00e4higkeiten im IT-Bereich weiter auszubauen.</p> <p>Meine besondere Leidenschaft gilt dem DevOps-Bereich und allem, was damit zusammenh\u00e4ngt. Ich finde es faszinierend, wie die enge Zusammenarbeit zwischen Softwareentwicklung und IT-Betrieb dazu beitr\u00e4gt, effizientere und stabilere Systeme zu schaffen. Dieses Interesse hat mich dazu veranlasst, mich intensiv mit den neuesten Technologien und Best Practices in diesem Bereich zu besch\u00e4ftigen.</p> <p>Insbesondere habe ich mich auf Kubernetes und Automatisierung fokussiert. Kubernetes ist eine leistungsstarke Open-Source-Container-Orchestrierungsplattform, die es erm\u00f6glicht, Anwendungen in grossem Umfang zuverl\u00e4ssig und skalierbar zu betreiben. Durch den Einsatz von Kubernetes und anderen Automatisierungswerkzeugen kann ich dazu beitragen, den Entwicklungs- und Bereitstellungsprozess zu optimieren und die Effizienz der IT-Systeme innerhalb der kantonalen Verwaltung zu steigern.</p> <p>In meiner Freizeit bin ich stets bem\u00fcht, mein Wissen und meine F\u00e4higkeiten in den Bereichen DevOps, Kubernetes und Automatisierung zu erweitern. Dazu besuche ich regelm\u00e4ssig Fachkonferenzen, beteilige mich an Online-Communities und arbeite an pers\u00f6nlichen Projekten, um meine Kenntnisse auf dem neuesten Stand zu halten.  </p>"},{"location":"Projekte/bbb-quotes/","title":"BBB-Quotes","text":"<p>Eines meiner favorisierten Projekte ist zweifelsohne \"BBB-Quotes\". In Zusammenarbeit mit einem Mitsch\u00fcler aus meiner Berufsfachschule initiierten wir dieses Vorhaben, um die herausragendsten Zitate unserer Lehrerinnen und Lehrer festzuhalten. Da unser Fachgebiet die Informatik ist, nutzten wir die Gelegenheit, um unsere Kenntnisse und F\u00e4higkeiten weiter auszubauen.</p> <p>Zum damaligen Zeitpunkt besch\u00e4ftigte ich mich intensiv mit der Erlernung von Kubernetes, einer container-orchestrierungsplattform, die die Bereitstellung, Skalierung und Verwaltung von containerisierten Anwendungen erleichtert. Daher entschied ich mich, die Implementierung von \"BBB-Quotes\" mithilfe von Kubernetes voranzutreiben, um sowohl praktische Erfahrungen zu sammeln als auch den Betrieb unserer Anwendung effizient zu gestalten.</p>"},{"location":"Projekte/bbb-quotes/#aufbau","title":"Aufbau","text":"<p>Um die Plattform erfolgreich in Betrieb zu nehmen, entschied ich mich f\u00fcr die Anmietung eines Cloud-Servers bei IONOS. Nach einer gewissen Zeit entfernte ich die Docker-Services von diesem Server und installierte stattdessen MikroK8s, eine leichtgewichtige Kubernetes-Distribution, die sich ideal f\u00fcr den Einsatz auf solchen Cloud-Servern eignet.</p> <p>Bei der Entwicklung von \"BBB-Quotes\" entschieden wir uns f\u00fcr die Verwendung des Content Management Systems (CMS) \"Ghost\". Ghost ist ein modernes, flexibles und benutzerfreundliches CMS, das sich besonders f\u00fcr das Ver\u00f6ffentlichen und Verwalten von Inhalten eignet. Durch die Integration von Ghost in unsere Plattform konnten wir eine ansprechende Benutzeroberfl\u00e4che erstellen und eine effiziente Verwaltung der gesammelten Zitate gew\u00e4hrleisten.</p> <p>Anschliessend erstellte ich die Deployment-YAML-Dateien f\u00fcr das \"BBB-Quotes\"-Projekt. Diese Dateien werden mithilfe von Argo CD, einem Continuous Delivery-Tool, automatisch abgerufen und auf dem Server bereitgestellt. Dadurch ist die Webseite unter der URL \"https://bbb-quotes\" zug\u00e4nglich.</p> <p>Um eine sichere Verbindung zu gew\u00e4hrleisten, nutze ich den Caddy Ingress Controller in Kombination mit Let's Encrypt, um SSL-Zertifikate f\u00fcr die Plattform zu beziehen. Dies erm\u00f6glicht es, die Kommunikation zwischen den Benutzern und der Webseite durch eine verschl\u00fcsselte Verbindung zu sch\u00fctzen und gleichzeitig die Vertrauensw\u00fcrdigkeit der Plattform zu erh\u00f6hen.  </p>"},{"location":"Projekte/bbb-quotes/#argocd","title":"Argocd","text":""},{"location":"Projekte/portal-mit-kubernetes/","title":"Portal mit Kubernetes","text":"<p>In diesem Projekt habe ich mir vorgenommen, das Portal der IT-Berufsbildung mit Kubernetes in Betrieb zu nehmen. Dies schien f\u00fcr mich die perfekte Aufgabe zu sein, da das Portal aus einer simplen Webapplikation ohne Umgebungsvariablen, mehreren APIs, die mit vielen Umgebungsvariablen beinhalten und einer Stateful Datenbank besteht. Das Portal wollte ich nicht mit allen APIs hochfahren, da diese den Rahmen des Projektes gesprengt h\u00e4tte. Daher habe ich mir konkret folgendes vorgenommen:</p> <ul> <li>User Interface Deployen</li> <li>Base API Deployen<ul> <li>ConfigMaps</li> </ul> </li> <li>Secrets</li> <li>Planning API Deployen<ul> <li>ConfigMaps</li> </ul> </li> <li>Secrets</li> <li>Redis Datenbank Deployen<ul> <li>StatefulSet</li> </ul> </li> </ul> <p></p> <p>UI und APIs, sowie die Datenbank sind nicht im selben Namespace, da ich ausprobieren wollte, wie sich Kubernetes verh\u00e4lt, wenn die Ressourcen in verschiedenen Namespaces sind.</p>"},{"location":"Projekte/portal-mit-kubernetes/#user-interface","title":"User Interface","text":"<p>F\u00fcr das UI war in diesem Projekt sicherlich der einfachste Teil.  Um \u00fcberhaupt auf das Image in unserer privaten Registry zugreifen zu k\u00f6nnen, musste ich ein Image Pull Secret definieren. Dies habe ich nach offizieller Kubernetes Dokumentation gemacht. Anschliessend konnte ich mein Deployment Script schreiben.</p>"},{"location":"Projekte/portal-mit-kubernetes/#deployment","title":"Deployment","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: portal-ui\nnamespace: portal\nspec:\nselector:\nmatchLabels:\napp: portal-ui\ntemplate:\nmetadata:\nlabels:\napp: portal-ui\nspec:\ncontainers:\n- name: portal-ui\nimage: registry.gitlab.com/it-berufsbildung-projekte/docker-images/kubernetes/portal-ui:latest\nresources:\nlimits:\nmemory: \"512Mi\"\ncpu: \"500m\"\n# Jeder Request bekommt 200 Mili-Cores\nrequests:\ncpu: 200m\nports:\n- containerPort: 80\n# Secret f\u00fcr die private Registry\nimagePullSecrets:\n- name: regcred\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: portal-ui\nspec:\nselector:\napp: portal-ui\nports:\n- port: 80\ntargetPort: 80\n</code></pre>"},{"location":"Projekte/portal-mit-kubernetes/#ingress","title":"Ingress","text":"<p>Um auf das Portal auch extern zugreifen zu k\u00f6nnen, habe ich eine Ingress Regel definiert. Man h\u00e4tte theoretisch den Service auch mittels NodePort freigeben k\u00f6nnen, der Ingress ist jedoch die deutlich sch\u00f6nere L\u00f6sung. Alle HTTP Anfragen landen zuerst bei unserem Reverseproxy Caddy. Dieser vergibt den Diensten ein SSL Zertifikat und leitet sie an das Cluster weiter, wo sie anhand der Ingress Regeln weitergeleitet werden. </p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: portal-ui-ingress\nnamespace: portal\nannotations:\nkubernetes.io/ingress.class: nginx\nspec:\nrules:\n- host: kubernetes.labor.it-berufsbildung.ch\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: portal-ui\nport: number: 80\n</code></pre>"},{"location":"Projekte/portal-mit-kubernetes/#hpa","title":"HPA","text":"<p>Mit der horizontalen Pod Autoskalierung konnte ich eine Regel definieren, bei der automatisch je nach Auslastung mehr Pods hochgefahren werden, oder auch \u00fcberfl\u00fcssige gel\u00f6scht werden. Hierf\u00fcr musste ich ebenfalls nochmals ein Configfile schreiben.</p> <p><pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\nname: portal-ui\nnamespace: portal\nspec:\nscaleTargetRef:\napiVersion: apps/v1\nkind: Deployment\nname: portal-ui minReplicas: 2\nmaxReplicas: 10\nmetrics:\n- type: Resource\nresource:\nname: cpu\ntarget:\ntype: Utilization\naverageUtilization: 50\n</code></pre> Mit dieser Konfiguration wird das Deployment <code>portal-ui</code> automatisch hochskaliert, nachdem ein Pod mehr als 50 % ausgelastet ist. Hierbei m\u00fcssen immer mindestens 2 Instanzen laufen und es k\u00f6nnen maximal 10 Instanzen hochgefahren werden. </p> <p>Somit hatte ich nun also das UI deployt und konnte mich an das Umsetzen der Datenbank k\u00fcmmern.</p>"},{"location":"Projekte/portal-mit-kubernetes/#redis-datenbank","title":"Redis Datenbank","text":"<p>Um die Redis Datenbank zu deployen, musste ich ein StatefulSet verwenden. Dieses wird immer dann gebraucht, wenn eine Stateful Applikation deployt wird. </p>"},{"location":"Projekte/portal-mit-kubernetes/#statefulset","title":"StatefulSet","text":"<p><pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\nname: redis\nspec:\nselector:\nmatchLabels:\napp: redis\nserviceName: redis\ntemplate:\nmetadata:\nlabels:\napp: redis\nspec:\ncontainers:\n- name: redis\nimage: dockermike1809/itbb-redis\nports:\n- containerPort: 6379\nvolumeMounts:\n- mountPath: /data\nname: redis-data\nresources:\nlimits:\nmemory: \"512Mi\"\ncpu: \"500m\"\nvolumes:\n- name: redis-data\nnfs:\nserver: 192.168.21.181\npath: /mnt/Data/k8s-redis\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: redis\nspec:\nselector:\napp: redis\nports:\n- port: 6379\n</code></pre> Hier musste ich nun mein erstes Volume verwenden. Daf\u00fcr habe ich auf unserem NAS einen NFS Share erstellt, der anschliessend direkt an den Pods angeh\u00e4ngt wird. Somit sind meine Daten sicher ausserhalb des Clusters aufbewahrt.</p>"},{"location":"Projekte/portal-mit-kubernetes/#replikation","title":"Replikation","text":"<p>Da das Replizieren einer Datenbank immer eine grosse Herausforderung ist, hatte ich auch hier deutlich M\u00fche. Ich habe versucht, das Ganze anhand einer Anleitung, die ich im Internet gefunden habe, umzusetzen.  Die Datenbanken sind zwar alle hochgekommen, sie schienen auch synchronisiert zu sein, jedoch nachdem ich versucht hatte etwas auf die Datenbanken zu schreiben, gab es Konflikte und jede Datenbank war auf einem anderen Stand. Daher habe ich mich dazu entschieden, die Skalierung der Datenbank wegzulassen und mit \"nur\" einer Instanz der Datenbank weiterzumachen.</p> <p>Nachdem meine Datenbank erfolgreich deployt wurde, konnte ich mich mit dem Deploy der ersten API auseinandersetzen.</p>"},{"location":"Projekte/portal-mit-kubernetes/#base-api","title":"Base API","text":"<p>Die Base API kam mit vielen Umgebungsvariablen daher. Da ich diese in einer ConfigMap und in einem Secret definiert habe, wurde das Deployment Script relativ lange.</p>"},{"location":"Projekte/portal-mit-kubernetes/#configmap","title":"ConfigMap","text":"<p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: base-api-configmap\ndata:\n# URL, auf der die Base-API zu finden ist\nBASE_API_URL: https://portal-api.kubernetes.labor.it-berufsbildung.ch\n# URL des Redis Services. Somit weiss die API, wo die Datenbank liegt.\nREDIS_URL: redis.default.svc.cluster.local REDIS_PORT: \"6379\"\nREDIS_DATABASE: \"1\"\nREDIS_PASSWORD: 1234\nCORS_ORIGIN: \"*\"\nLOG_LEVELS: '[\"verbose\",\"debug\",\"log\",\"warn\",\"error\"]'\nAPI_PROTOCOL: http\n# Anders als bei Docker, ist der Container bzw. der Podname nicht fix.\n# Daher musste ich 0.0.0.0 nehmen, damit die API auf jedem Interface h\u00f6rt.\nAPI_HOST: 0.0.0.0 API_PORT: \"3331\"\nENVIRONMENT: dev\nAUTH_ISSUER: https://it-berufsbildung.eu.auth0.com\nAUTH_AUD: portal@it-berufsbildung.ch\nSERVER_PROTOCOL: https\nSERVER_HOST: portal-api.kubernetes.labor.it-berufsbildung.ch\nSERVER_PORT: \"443\"\n</code></pre> Somit hatte ich die nicht \"sch\u00fctzenswerten Variablen\" definiert. Da ich aber auch noch die Client-Informationen f\u00fcr Auth0 irgendwo unterbringen musste, musste noch ein Secret erstellt werden. </p>"},{"location":"Projekte/portal-mit-kubernetes/#secret","title":"Secret","text":"<p><pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: base-secret\ntype: Opaque\ndata:\nAUTH_CLIENT_ID: base64-kodierte-werte\nAUTH_CLIENT_SECRET: base64-kodierte-werte\n</code></pre> Mit dem Secret hatte ich jetzt alle Umgebungsvariablen gesetzt und es war an der Zeit, das Deployment zu schreiben :tada:.</p>"},{"location":"Projekte/portal-mit-kubernetes/#deployment_1","title":"Deployment","text":"<p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: base-api\nspec:\nselector:\nmatchLabels:\napp: base-api\ntemplate:\nmetadata:\nlabels:\napp: base-api\nspec:\ncontainers:\n- name: base-api\nimage: registry.gitlab.com/it-berufsbildung-projekte/docker-images/int/portal-api:latest\nresources:\nlimits:\nmemory: \"512Mi\"\ncpu: \"500m\"\nrequests:\ncpu: 200m\nvolumeMounts:\n- mountPath: /usr/src/app/dist/import-data/\nname: base-api-volume\nports:\n- containerPort: 3331\nenv:\n- name: AUTH_ISSUER\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: AUTH_ISSUER\n- name: BASE_API_URL\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: BASE_API_URL\n- name: REDIS_URL\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: REDIS_URL\n- name: REDIS_PORT\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: REDIS_PORT\n- name: REDIS_DATABASE\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: REDIS_DATABASE\n- name: CORS_ORIGIN\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: CORS_ORIGIN\n- name: LOG_LEVELS\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: LOG_LEVELS\n- name: API_PROTOCOL\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: API_PROTOCOL\n- name: API_HOST\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: API_HOST\n- name: API_PORT\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: API_PORT\n- name: ENVIRONMENT\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: ENVIRONMENT\n- name: AUTH_AUD\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: AUTH_AUD\n- name: AUTH_CLIENT_ID\nvalueFrom:\nsecretKeyRef:\nname: base-secret\nkey: AUTH_CLIENT_ID\n- name: AUTH_CLIENT_SECRET\nvalueFrom:\nsecretKeyRef:\nname: base-secret\nkey: AUTH_CLIENT_SECRET\n- name: REDIS_PASSWORD\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: REDIS_PASSWORD\n- name: SERVER_PROTOCOL\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: SERVER_PROTOCOL\n- name: SERVER_HOST\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: SERVER_HOST\n- name: SERVER_PORT\nvalueFrom:\nconfigMapKeyRef:\nname: base-api-configmap\nkey: SERVER_PORT\nimagePullSecrets:\n- name: regcred\nvolumes:\n- name: base-api-volume\nnfs:\nserver: 192.168.21.181\npath: /mnt/Data/k8s-base-api\n---\napiVersion: v1\nkind: Service\nmetadata:\nname: base-api\nspec:\nselector:\napp: base-api\nports:\n- port: 3331\ntargetPort: 3331\n</code></pre> Auch hier wird wieder auf unser NAS zugegriffen, damit die API an die Import-Daten kommt. </p>"},{"location":"Projekte/portal-mit-kubernetes/#ingress_1","title":"Ingress","text":"<p>Da die API von aussen erreicht sein sollte, musste wieder ein Ingress Regel geschrieben werden.  </p> <p><pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: base-api-ingress\nannotations:\nkubernetes.io/ingress.class: nginx\nlabels:\nname: base-api-ingress\nspec:\nrules:\n- host: portal-api.kubernetes.labor.it-berufsbildung.ch\nhttp:\npaths:\n- pathType: Prefix\npath: \"/\"\nbackend:\nservice:\nname: base-api\nport: number: 3331\n</code></pre> Die Regel ist vom Aufbau her gleich wie beim UI. </p> <p>Somit war nun also auch die Base API einsatzbereit :triumph:</p>"},{"location":"Projekte/portal-mit-kubernetes/#planning-api","title":"Planning API","text":"<p>Da die Planning API vom Aufbau her exakt gleich ist wie die Base-API, werde ich das Deployment hier nicht genauer ausf\u00fchren.</p>"},{"location":"Projekte/portal-mit-kubernetes/#fazit","title":"Fazit","text":"<p>Ich bin sehr erstaunt, dass ich dieses Projekt tats\u00e4chlich hingebracht habe und das auch noch in so kurzer Zeit. Stolz auf mich bin ich sicherlich und meiner Meinung nach habe ich dieses Projekt sehr viel Willensst\u00e4rke bewiesen, da ich auch bei gescheiterten Versuchen nie aufgegeben habe.</p> <p></p>"},{"location":"Projekte/portal-mit-kubernetes/#positives","title":"Positives","text":"<p>In diesem Projekt hatte ich sehr viel Spass und ich bin extrem Stolz auf mich, dass ich das Projekt in solch kurzer Zeit realisieren konnte. Ich konnte ich weniger als einer Woche mein Projekt Planen und realisieren. </p>"},{"location":"Projekte/portal-mit-kubernetes/#negatives","title":"Negatives","text":"<p>Ich habe ziemlich mit dem DNS-Server der Pods gek\u00e4mpft. Damit die Pods sich bei Auth0 registrieren k\u00f6nnen, muss diese Dom\u00e4ne vom Core DNS aufgel\u00f6st werden k\u00f6nnen. Da dies bei mir aber nicht funktioniert hat, musste ich mir eine L\u00f6sung im Internet zusammensuchen.  Schlussendlich habe ich herausgefunden, dass ich mittels einer ConfigMap den DNS-Server, mit dem der Core DNS Dienst selbst seine Anfragen aufl\u00f6st, setzen kann.  </p> <p><pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: coredns\nnamespace: kube-system\ndata:\nCorefile: |\n.:53 {\nerrors\nhealth {\nlameduck 5s\n}\nready\nkubernetes cluster.local in-addr.arpa ip6.arpa {\npods insecure\nfallthrough in-addr.arpa ip6.arpa\nttl 30\n}\nprometheus :9153\n# Hier kann der DNS Server gesetzt werden\nforward . 8.8.8.8\ncache 30\nloop\nreload\nloadbalance\n}\n</code></pre> Nachdem man diese ConfigMap erstellt wurde, muss der Core DNS Dienst neu gestartet werden. Da man in Kubernetes keine Pods in dem Sinn neu starten kann, m\u00fcssen diese gel\u00f6scht werden, damit anschliessend neue Pods mit der neuen Konfiguration hochkommen. Daf\u00fcr kann man folgenden Befehl verwenden:</p> <p><code>kubectl delete pods coredns-&lt;podname pod1&gt; coredns-&lt;podname pod2&gt; -n kube-system</code></p>"}]}